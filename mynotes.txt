Deep learning
========

2016-01-13

Verena Kaynig-Fittkau, github.com/vkaynig/

Slides are in: Deep Learning â€“ Part 1.pdf in the github repo.

Intro: Why
========

Why deep learning? It works. 

Google Brain found there were a lot of cats, a lot of human faces on
youtube. Really unsupervised. NYTIMES 2012-06-26. Not 'grad student
descent' which is what we used to have. Computer Vision Conferences
are boring: now everybody says "we trained a convolutional...." You
need hand designed features to train an SVM or whatever: a crude form
of expert knowledge. Her grad stud project was detect neurons in
electron microscopy. Tantalizing thing: I can see the neurons myself,
so if I can just find the right features, the SVM will win! Deep
learning is so called because several layers of features: edge
detector, nose detector, face detector.

"Recurrent" ANNs, sez recommended for NLP or time series.

perceptron BBC documentary. 400 pixel sensor. Guy sitting there
training with paper photos, and two switches. WRONG/RIGHT. MAN/WOMAN.

SVM, RF, etc developed to deal with small data sets. And you need an
annotator for ground truth. Don't do deep learning < 60,000 training
examples (because other ML algos will outperform). DL would also be
useful to leverage unlabeled data.

More data more compute power these days. Supposedly can get just 1 or
2 or 3 nvidia gpus and be close to n cores of Google Brain.

Q/A: sort of did dev thing Spearmint to determine hyperparams like
learning rate, etc, via Gaussian Processes.

WHAT is deep learning then?
========

3x3 image --> 9-dim vector. No sense of "this pixel is above this
one." Basics of ANNs, weights, bias unit, etc.... fire = s(b + w' *
x). Where s(x) = sigmoid = 1/(1+exp(-cx)), kinda a substitute for step
func. Then do W*x where W is matrix and w was a vector. For middle
steps use s(). For last step use G().

Nice thing: in theano we don't have to compute the gradient (and tell
it to the software).

*Interesting blackboard bit:* w and b are 2 vectors that define a
hyperplane. Each image is a point (in N-dim space). Matrix W is a
bunch of hyperplanes. In training, the (not yet) separating
hyperplanes wriggle and shift til they hopefully do separate the
classes. More efficient if you normalize your data. That is, you want
the random initial weights to kind of land in the middle of your
classes, so they don't have to shift and shift and shift all the same
direction toward the bulk of your data.

argmin_theta sum l(f(x, theta), y)

x: features, theta: weights etc, y: ground truth train lables, f():
applies theta to x to get yhat, l(): loss function. 

stochastic gradient descent. Learning rate (step size) is (the most)
important param to tune. We don't care that much about finding global
min in fact. We want a decent local min that we can get to kind of
fast. Smaller rate is more stable but slower. Yes, there are other
things other than stochastic gradient descent.

"Labels define your energy landscape and define what's better."

Google Brain used "autoencoder". Idea is to reproduce input. It is
supervised ML in the sense that it has labels, and the Input *is* the
label.

MNIST digit recognition. 

See also torch, caffe, tensorflow (besides theano). She likes that
Theano has transparent GPU integration. "high level on top": pylearn2.
hard to install! or keras, lasagne, blocks. Torch = google deep mind,
facebook ai, ibm. uses Lua, no python. caffe = c++, has "zoo" with
pre-trained, like a model that already will do face recognition.
tensorflow she says has good viz but performance issues. She hasn't
switched. but worth keeping an eye on. 

Misc QA
========

Why not python 3? Theano is really painful on windows + GPU + python
3. 

Also interesting perspective of "if you use notebook, your notebook
can crash, so consider just plain python .py files."

Interactive part
========

[FYI all of this is really just logistic regression]

Theano shared var = "data can be shared between the CPU and the GPU."
So there is a .get_value() method on each one of those thingies. But
train_set_y doesn't even have .get_value(). Why? See ipy notebook.

*exercise:* use np.mean and np.std

solutions are in 2nd ver of the notebook called "executed"

Note if you're "into performance" then don't do get_value or eval
early on. But can also cause problems about recomputing same thing
again and again. All due to the "computational graph." 

[note, I think that once you do the mean / std dev thing, then you
have to do `.eval()` instead of get_value. So inside your
tile_raster_images function it'll have to do that.

Resume
-------

Theano functions are another mysterious thing, besides shared
variables. 

Vars work like "this variable is a matrix of floats, this one is a
vector of ints." Placeholders for your training data. Lets you make a
compu graph for this network which has placeholders, and then new data
can go in. 

np.zeros((28*28, 10), ...) because there are 10 classes (10 digits).
name='W' is mainly for debugging. borrow=True is kind of tricky. Only
set to true because deep learning tutorial says it's OK. You really
want to spend a week getting CUDA to work with it because then you can
train in hours instead of days. 

`error` in the code is mean misclassification rate. But gradient of
that is nasty. So look at `error` but train with `cost`. `p_y_given_x`
is a 10-vector. (Incidentally all 10 elements sum to 1.) There is a
fair amount of "indexing magic" based on the whole idea that we're
classifying digits. Earlier we called cost function "l" or the loss
function. This code uses log(P(y|x)) AKA log likelihood. You can use
other options for your cost function, stuff like cross entropy.

[All this Theano stuff with `=` signs just builds the graph. Doesn't
actually *do* anything.] `T.grad(cost=cost, wrt=W)` ... the "wrt" part
means "with respect to." 0.001 is more realistic learning rate.

Finally `theano.function`. **Take home point** is understanding
`theano.function` and shared variables.

"Epoch" is deep learning vocab. 10 epochs may not cut it. Bad
performance because in *each* iteration you're calling the `.eval()`
which is copying things from GPU to CPU. So instead copy things over
*just once* by saying `foo = blah.eval()` and then referring to `foo`
forever onwards.

**exercise:** write a Theano func for basically y hat. We didn't do
this exercise.

Function that takes a shared variable
--------

Going to have code that looks like `theano.function(inputs=[],`....
Where inputs is empty. Put the shared var into the `givens` parameter.
Also now you are going to call the function like `train()`, and that's
it. 

Batch training: [sounds like kinda cheating] which is just estimating
the gradient off of a sample, not off of the whole mess of examples.
Screws you up if your data is sorted, so randomly shuffle your
training examples before doing batch training.

All the rest of this tutorial is sort of extra.

Learning Rate Decay, and Q/A
========

Good to tune just this one parameter.

Discussion of deep vs wide. Can in theory do any function with just a
nearly infinitely wide layer. When you go from 1 to 2 layers, you
could emulate that with just exponentially wider single layer. So
that's why most people go deeper. 

Exanple number of nodes seen in practice like 2000 2500 500. 50
something layers has been done.

Yes, we've only done one layer so far. But the adding layers is easy.
You already know about `givens` etc., etc.

Can play around with activation functions other than sigmoid.
"rectified linear function" is max(x,0). 

QA: does theano have nice wrapper functions for things like cross
validation error?

Can play with different initializations.

When you do visualizing "hidden Unit Activations", you want it to look
random. Stripes means 2 units the same, you failed to break symmetry.
Black bar means you killed that unit.
